{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras MNIST Guided Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras for neural networks - guided example\n",
    "- Digit recognition\n",
    "    - classic machine learning problem\n",
    "    - MNIST dataset\n",
    "- Create 3 different styles of networks, discuss advanages and disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data \n",
    "- values of pixels\n",
    "- not a single table, different, higher dimensionality structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Import various components for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 9s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron\n",
    "- set of perceptron models organized into layers\n",
    "- one layer feeds the next\n",
    "- Need to reshape data into flat vectors for each digit\n",
    "- conver outcome to matrix of binary variables, rather than the digit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Change shape\n",
    "# Note that our images are 28x28 pixels, so in reshaping to arrays we want\n",
    "# 60000 arrays of length 784, one for each image\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# Convert to float32 for type consistency\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize values to 1 from 0 to 255 (256 values of pixels)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Print sample sizes\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "# instead of one column with 10 values, create 10 binary columns\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create model\n",
    "- Use dense layers and dropouts\n",
    "    - dropout drops a certain portion of our perceptrons in order to prevent overfitting\n",
    "    - actitation function, relu, Rectified Linear Unit\n",
    "- Using 64 perceptron wide layers, relatively arbitrary, units with 2^x parallelize more efficiently.\n",
    "- Our number of parameters is the product of our input width plus one and our layer width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the\n",
    "# first layer\n",
    "# Relu is the activation function\n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/crystal/Library/Python/3.7/lib/python/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.4340 - acc: 0.8714 - val_loss: 0.1954 - val_acc: 0.9401\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2033 - acc: 0.9393 - val_loss: 0.1417 - val_acc: 0.9582\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1565 - acc: 0.9529 - val_loss: 0.1146 - val_acc: 0.9643\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1305 - acc: 0.9602 - val_loss: 0.1053 - val_acc: 0.9681\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1148 - acc: 0.9651 - val_loss: 0.0998 - val_acc: 0.9712\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1040 - acc: 0.9678 - val_loss: 0.0962 - val_acc: 0.9722\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0957 - acc: 0.9716 - val_loss: 0.0856 - val_acc: 0.9741\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0892 - acc: 0.9730 - val_loss: 0.0851 - val_acc: 0.9753\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0841 - acc: 0.9741 - val_loss: 0.0896 - val_acc: 0.9740\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0798 - acc: 0.9754 - val_loss: 0.0892 - val_acc: 0.9740\n",
      "Test loss: 0.08917658618534915\n",
      "Test accuracy: 0.974\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "- This code will take serious time to run\n",
    "- Convolution \n",
    "    - takes your data and creates overlapping subsegments testing for a given feature\n",
    "    - first define shape of data\n",
    "        - 2d for our example since images are 2d\n",
    "    - create tiles, called kernels\n",
    "        - like little windows look over subsets of the data of a given size\n",
    "        - example below has 3x3 kernels (convolutional layer)\n",
    "        - example below has 2\n",
    "    - pooling layer\n",
    "        - downsampling technique\n",
    "        - reduces sample size and simplifies later processes\n",
    "        - looks over grid in non-overlapping segments\n",
    "    - after pooling flatten data back out so it can be put into dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.2654 - acc: 0.9177 - val_loss: 0.0610 - val_acc: 0.9808\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0901 - acc: 0.9738 - val_loss: 0.0436 - val_acc: 0.9846\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0659 - acc: 0.9807 - val_loss: 0.0363 - val_acc: 0.9880\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0545 - acc: 0.9838 - val_loss: 0.0337 - val_acc: 0.9886\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0471 - acc: 0.9855 - val_loss: 0.0324 - val_acc: 0.9891\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0403 - acc: 0.9875 - val_loss: 0.0280 - val_acc: 0.9909\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0369 - acc: 0.9887 - val_loss: 0.0302 - val_acc: 0.9897\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0334 - acc: 0.9897 - val_loss: 0.0287 - val_acc: 0.9909\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0301 - acc: 0.9904 - val_loss: 0.0293 - val_acc: 0.9907\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0289 - acc: 0.9914 - val_loss: 0.0276 - val_acc: 0.9906\n",
      "Test loss: 0.027621145403021\n",
      "Test accuracy: 0.9906\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions, from our data\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Recurrent Neural Networks\n",
    "- Lets data cycle through the network\n",
    "- Data flows in more than one direction \n",
    "- Use recurrent layers and often time distribution\n",
    "- Examples:\n",
    "    - https://github.com/keras-team/keras/blob/master/examples/mnist_hierarchical_rnn.py\n",
    "- Even slower than other neural networks\n",
    "- LTSM = Long short-term memory\n",
    "    - applicable to tasks such as unsegmented, connected handwriting, speeech recognition\n",
    "    - each unit has a cell, input gate, output gate, and forget gate\n",
    "    - remembers values over an arbitrary time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 114s 2ms/step - loss: 1.0286 - acc: 0.6523 - val_loss: 0.5607 - val_acc: 0.8168\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 106s 2ms/step - loss: 0.4317 - acc: 0.8638 - val_loss: 0.3283 - val_acc: 0.8991\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.2568 - acc: 0.9215 - val_loss: 0.2588 - val_acc: 0.9218\n",
      "Test loss: 0.2588168480843306\n",
      "Test accuracy: 0.9218\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "\n",
    "# Embedding Dimensions\n",
    "row_hidden = 32\n",
    "col_hidden = 32\n",
    "\n",
    "# The data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshapes data to 4d for Hierarchical RNN\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Converts class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "row, col, pixel = x_train.shape[1:]\n",
    "\n",
    "# 4D input\n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# Encodes a row of pixels using TimeDistributed Wrapper\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# Encodes columns of encoded rows\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# Final predictions and model\n",
    "prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n",
    "model = Model(x, prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluation\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drill: 99% MLP\n",
    "We have the MLP above, which runs reasonably quickly. Copy that code down here and see if you can tune it to achieve 99% accuracy with a Multi-Layer Perceptron. Does it run faster than the recurrent or concolutional neural nets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_111 (Dense)            (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2873 - acc: 0.9151 - val_loss: 0.1513 - val_acc: 0.9534\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.1317 - acc: 0.9612 - val_loss: 0.1011 - val_acc: 0.9716\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.1000 - acc: 0.9701 - val_loss: 0.0871 - val_acc: 0.9739\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0838 - acc: 0.9749 - val_loss: 0.0826 - val_acc: 0.9760\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0732 - acc: 0.9786 - val_loss: 0.0830 - val_acc: 0.9759\n",
      "Epoch 6/10\n",
      "54144/60000 [==========================>...] - ETA: 0s - loss: 0.0644 - acc: 0.9813"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Change shape \n",
    "# Note that our images are 28*28 pixels, so in reshaping to arrays we want\n",
    "# 60,000 arrays of length 784, one for each image\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# Convert to float32 for type consistency\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize values to 1 from 0 to 255 (256 values of pixels)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Print sample sizes\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "# So instead of one column with 10 values, create 10 binary columns\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "#model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
